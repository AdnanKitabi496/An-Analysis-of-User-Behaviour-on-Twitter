{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fe468d",
   "metadata": {},
   "source": [
    "# Infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ebcc5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-166777ce9d1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0migraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\igraph\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \"\"\"\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m from igraph._igraph import (\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mADJ_DIRECTED\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mADJ_LOWER\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import igraph\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Saved the dataframe after added the \"Tweet_From\" column and reading the file here.\n",
    "data_language_german = pd.read_csv(\"colab_3Dec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = \"test.gml\"  #File path and name to save the gml file\n",
    "edges = [] \n",
    "nodes = set()\n",
    "tf_from = [tf for tf in data_language_german[\"Tweet_From\"]]\n",
    "edges = zip(data_language_german['Tweet_From'],data_language_german['Screen Name'])\n",
    "#edges += [(follower, username) for follower in tf_from]\n",
    "nodes.update(tf_from)\n",
    "edges = list(set(tuple(sorted(edge)) for edge in edges))\n",
    "nodes = list(set(nodes))\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)\n",
    "nx.write_gml(G, graph_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b10e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r'\\\\'\n",
    "def save(g, clusters, filename):\n",
    "    print('Saving...', filename)\n",
    "    membership = clusters.membership\n",
    "    writer = csv.writer(open(DATA_DIR+filename, \"w\"))\n",
    "    writer.writerow(['Id', 'Label', 'Modularity Class'])\n",
    "    for id, name, memb in zip(g.vs[\"id\"], g.vs[\"label\"], membership):\n",
    "        writer.writerow([id, name, memb])\n",
    "\n",
    "\n",
    "graph_file_name = r'C:\\Users\\RR510300\\Downloads\\block\\30_Nov_2021\\test\\test.gml' ## Read the .gml file\n",
    "g = igraph.read(graph_file_name)\n",
    "\n",
    "lp_csv = \"lp.csv\"\n",
    "lp = g.community_label_propagation()  # Using Community label propagation\n",
    "save(g, lp, lp_csv)\n",
    "\n",
    "ml_csv = \"ml.csv\"\n",
    "ml = g.community_multilevel()  # Using Community multilevel\n",
    "save(g, ml, ml_csv)\n",
    "\n",
    "im_csv = 'im.csv'\n",
    "im = g.community_infomap()     #Using Infomap\n",
    "save(g, im, im_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4fb7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5d74478",
   "metadata": {},
   "source": [
    "# Network analysis using betweenness_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script works on 2 columns in the tweets file received through the Twitter API: \" screen_name\" and \" text\" (note the blankspaces inserted by Twitter)\n",
    "# To make it work with the output of GetOldTweets3, change the label username to screenname with a leading blankspace, and add a leading blankspace to the label text\n",
    "# Line 94 has the input file name tweets.csv (change it as needed)\n",
    "# Output file network_edges.csv has two columns.\n",
    "# The first (left) column is the target and the second (right) column is the source in a directed network\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Definition for strippping whitespace\n",
    "def trim(dataset):\n",
    "    trim = lambda x: x.strip() if type(x) is str else x\n",
    "    return dataset.applymap(trim)\n",
    "\n",
    "def construct_edges(tweeters):\n",
    "    \"\"\"\n",
    "    :param tweeters: a datafrmae of twitter handles\n",
    "    :return: A dataframe of edges with 'handles of retweeted and reply/mentions' as first column\n",
    "    and the twitter of handle of the tweeted person as the second column.\n",
    "    \"\"\"\n",
    "    network_edges = []\n",
    "    for idx, row in tweeters.iterrows():\n",
    "        for name in list(np.array(row.rt_name).flat):\n",
    "            network_edges.append({'rt_name': row['Screen Name'], ' screen_name': name })\n",
    "    network_edges = pd.DataFrame(network_edges)\n",
    "    return network_edges\n",
    "\n",
    "\n",
    "def get_rt_names(tweets):\n",
    "  \n",
    "    import re\n",
    "\n",
    "    def get_names(tweet):\n",
    " \n",
    "        pattern = re.compile('^@.*')\n",
    "        text = tweet['Tweet Content']\n",
    "        text_tokens = str(text).split()\n",
    "        screen_name = tweet['Screen Name']\n",
    "        matches = []\n",
    "        for tok in text_tokens:\n",
    "            match_name = pattern.match(tok)\n",
    "            if (match_name):\n",
    "                rt_name = match_name.group()\n",
    "                rt_name = re.sub('[^\\w]', \"\", rt_name)\n",
    "                matches.append(rt_name)\n",
    "        if not matches:\n",
    "            matches.append(screen_name)\n",
    "        return matches\n",
    "\n",
    "    if 'Screen Name' in tweets.columns:\n",
    "        if 'Tweet Content' in tweets.columns:\n",
    "            tweets['rt_name'] = tweets.apply(get_names, axis=1)\n",
    "        else:\n",
    "            print(\"please check for the column names\")\n",
    "            print(\" 'Tweet Content': column name for the tweet text is missing\")\n",
    "            print(\" you might check for extra white spaces as well\")\n",
    "    else:\n",
    "        print(\"please check for the column names\")\n",
    "        print(\" ' screen_name': column name for the twitter profle name is missing\")\n",
    "        print(\" you might check for extra white spaces as well\")\n",
    "    tweeters = tweets[[ 'rt_name', 'Screen Name']]\n",
    "    return tweeters\n",
    "\n",
    "\n",
    "def read_csv():\n",
    "    \"\"\"\n",
    "    :return: tweets file in latin1 encoding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tweets = pd.read_csv(r\"C:\\Users\\RR510300\\Downloads\\block\\30_Nov_2021\\test\\colab_3Dec.csv\", encoding='latin1')\n",
    "\n",
    "    except:\n",
    "        print(\"File not found\")\n",
    "        print(\"Please check if the file named 'tweets.csv' exists in the directory:\")\n",
    "        print(os.getcwd() + \"/\")\n",
    "        exit()\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    :tweeters: two column file with screen name of tweet and list of retweet names for each tweet\n",
    "    :network_edges: created by taking tweeters and turning it into edgelist\n",
    "    :network_edges.to_csv: outputs dataframe to file\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    tweets = read_csv()\n",
    "    if tweets is None:\n",
    "        return\n",
    "    screen_name_list = []\n",
    "    rt_name_list = []\n",
    "    tweeters = get_rt_names(tweets)\n",
    "    for row in range(0,len(tweeters['Screen Name'])):\n",
    "        screen_name = tweeters['Screen Name'][row]\n",
    "        for retweet in tweeters['rt_name'][row]:\n",
    "            screen_name_list.append(screen_name)\n",
    "            rt_name_list.append(retweet)     \n",
    "    network_edges = pd.DataFrame()\n",
    "    network_edges['rt_name'] = rt_name_list\n",
    "    network_edges['Screen Name'] = screen_name_list\n",
    "    # file_name = str(os.getcwd()) + \"\\\\\" + \"network_edges\" + \"_\" + str(datetime.now()) + \".csv\"\n",
    "    network_edges.to_csv(\"output.csv\", index=False)\n",
    "    print(\"edges file is ready @: \" + str(os.getcwd()))\n",
    "\n",
    "# compile the file and run the function below.\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pandas_edgelist(\n",
    "    df,\n",
    "    source=\"source\",\n",
    "    target=\"target\",\n",
    "    edge_attr=None,\n",
    "    create_using=None,\n",
    "    edge_key=None,\n",
    "):\n",
    "    \"\"\"Returns a graph from Pandas DataFrame containing an edge list.\n",
    "\n",
    "    The Pandas DataFrame should contain at least two columns of node names and\n",
    "    zero or more columns of edge attributes. Each row will be processed as one\n",
    "    edge instance.\n",
    "\n",
    "    Note: This function iterates over DataFrame.values, which is not\n",
    "    guaranteed to retain the data type across columns in the row. This is only\n",
    "    a problem if your row is entirely numeric and a mix of ints and floats. In\n",
    "    that case, all values will be returned as floats. See the\n",
    "    DataFrame.iterrows documentation for an example.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "        An edge list representation of a graph\n",
    "\n",
    "    source : str or int\n",
    "        A valid column name (string or integer) for the source nodes (for the\n",
    "        directed case).\n",
    "\n",
    "    target : str or int\n",
    "        A valid column name (string or integer) for the target nodes (for the\n",
    "        directed case).\n",
    "\n",
    "    edge_attr : str or int, iterable, True, or None\n",
    "        A valid column name (str or int) or iterable of column names that are\n",
    "        used to retrieve items and add them to the graph as edge attributes.\n",
    "        If `True`, all of the remaining columns will be added.\n",
    "        If `None`, no edge attributes are added to the graph.\n",
    "\n",
    "    create_using : NetworkX graph constructor, optional (default=nx.Graph)\n",
    "        Graph type to create. If graph instance, then cleared before populated.\n",
    "\n",
    "    edge_key : str or None, optional (default=None)\n",
    "        A valid column name for the edge keys (for a MultiGraph). The values in\n",
    "        this column are used for the edge keys when adding edges if create_using\n",
    "        is a multigraph.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    to_pandas_edgelist\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Simple integer weights on edges:\n",
    "\n",
    "    >>> import pandas as pd\n",
    "    >>> pd.options.display.max_columns = 20\n",
    "    >>> import numpy as np\n",
    "    >>> rng = np.random.RandomState(seed=5)\n",
    "    >>> ints = rng.randint(1, 11, size=(3, 2))\n",
    "    >>> a = [\"A\", \"B\", \"C\"]\n",
    "    >>> b = [\"D\", \"A\", \"E\"]\n",
    "    >>> df = pd.DataFrame(ints, columns=[\"weight\", \"cost\"])\n",
    "    >>> df[0] = a\n",
    "    >>> df[\"b\"] = b\n",
    "    >>> df[[\"weight\", \"cost\", 0, \"b\"]]\n",
    "       weight  cost  0  b\n",
    "    0       4     7  A  D\n",
    "    1       7     1  B  A\n",
    "    2      10     9  C  E\n",
    "    >>> G = nx.from_pandas_edgelist(df, 0, \"b\", [\"weight\", \"cost\"])\n",
    "    >>> G[\"E\"][\"C\"][\"weight\"]\n",
    "    10\n",
    "    >>> G[\"E\"][\"C\"][\"cost\"]\n",
    "    9\n",
    "    >>> edges = pd.DataFrame(\n",
    "    ...     {\n",
    "    ...         \"source\": [0, 1, 2],\n",
    "    ...         \"target\": [2, 2, 3],\n",
    "    ...         \"weight\": [3, 4, 5],\n",
    "    ...         \"color\": [\"red\", \"blue\", \"blue\"],\n",
    "    ...     }\n",
    "    ... )\n",
    "    >>> G = nx.from_pandas_edgelist(edges, edge_attr=True)\n",
    "    >>> G[0][2][\"color\"]\n",
    "    'red'\n",
    "\n",
    "    Build multigraph with custom keys:\n",
    "\n",
    "    >>> edges = pd.DataFrame(\n",
    "    ...     {\n",
    "    ...         \"source\": [0, 1, 2, 0],\n",
    "    ...         \"target\": [2, 2, 3, 2],\n",
    "    ...         \"my_edge_key\": [\"A\", \"B\", \"C\", \"D\"],\n",
    "    ...         \"weight\": [3, 4, 5, 6],\n",
    "    ...         \"color\": [\"red\", \"blue\", \"blue\", \"blue\"],\n",
    "    ...     }\n",
    "    ... )\n",
    "    >>> G = nx.from_pandas_edgelist(\n",
    "    ...     edges,\n",
    "    ...     edge_key=\"my_edge_key\",\n",
    "    ...     edge_attr=[\"weight\", \"color\"],\n",
    "    ...     create_using=nx.MultiGraph(),\n",
    "    ... )\n",
    "    >>> G[0][2]\n",
    "    AtlasView({'A': {'weight': 3, 'color': 'red'}, 'D': {'weight': 6, 'color': 'blue'}})\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    g = nx.empty_graph(0, create_using)\n",
    "\n",
    "    if edge_attr is None:\n",
    "        g.add_edges_from(zip(df[source], df[target]))\n",
    "        return g\n",
    "\n",
    "    reserved_columns = [source, target]\n",
    "\n",
    "    # Additional columns requested\n",
    "    attr_col_headings = []\n",
    "    attribute_data = []\n",
    "    if edge_attr is True:\n",
    "        attr_col_headings = [c for c in df.columns if c not in reserved_columns]\n",
    "    elif isinstance(edge_attr, (list, tuple)):\n",
    "        attr_col_headings = edge_attr\n",
    "    else:\n",
    "        attr_col_headings = [edge_attr]\n",
    "    if len(attr_col_headings) == 0:\n",
    "        raise nx.NetworkXError(\n",
    "            f\"Invalid edge_attr argument: No columns found with name: {attr_col_headings}\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        attribute_data = zip(*[df[col] for col in attr_col_headings])\n",
    "    except (KeyError, TypeError) as e:\n",
    "        msg = f\"Invalid edge_attr argument: {edge_attr}\"\n",
    "        raise nx.NetworkXError(msg) from e\n",
    "\n",
    "    if g.is_multigraph():\n",
    "        # => append the edge keys from the df to the bundled data\n",
    "        if edge_key is not None:\n",
    "            try:\n",
    "                multigraph_edge_keys = df[edge_key]\n",
    "                attribute_data = zip(attribute_data, multigraph_edge_keys)\n",
    "            except (KeyError, TypeError) as e:\n",
    "                msg = f\"Invalid edge_key argument: {edge_key}\"\n",
    "                raise nx.NetworkXError(msg) from e\n",
    "\n",
    "        for s, t, attrs in zip(df[source], df[target], attribute_data):\n",
    "            if edge_key is not None:\n",
    "                attrs, multigraph_edge_key = attrs\n",
    "                key = g.add_edge(s, t, key=multigraph_edge_key)\n",
    "            else:\n",
    "                key = g.add_edge(s, t)\n",
    "\n",
    "            g[s][t][key].update(zip(attr_col_headings, attrs))\n",
    "    else:\n",
    "        for s, t, attrs in zip(df[source], df[target], attribute_data):\n",
    "            g.add_edge(s, t)\n",
    "            g[s][t].update(zip(attr_col_headings, attrs))\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad76a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "df = pd.read_csv('output.csv') #read in CSV file with edgelist\n",
    "df.columns = [\"source\", \"target\"]\n",
    "G = from_pandas_edgelist(df, create_using=nx.DiGraph()) #create graph from edgelist as a directed network (DiGraph())\n",
    "\n",
    "Indegrees = [val for val in G.in_degree().values()] #gets the in-degree of all nodes in the network\n",
    "Outdegrees = [val for  val in G.out_degree().values()] #gets the out-degree of all nodes in the network\n",
    "Nodes = G.nodes() #gets the name of all nodes in the network\n",
    "\n",
    "G = from_pandas_edgelist(df, create_using=nx.Graph()) #recreate the network as a undirected network to calculate betweenness score and plot\n",
    "\n",
    "Betweenness_dict = nx.betweenness_centrality(G, normalized=False) #gets the betweenness score for of all nodes in the network\n",
    "for node,value in Betweenness_dict.items():\n",
    "    Betweenness_dict[node] = value/2 #divide each betweenness score by 2 because in directed networks betweeness scores are half what they would be in undirected networks\n",
    "n_color = np.asarray([(Betweenness_dict[n] + 1) for n in Nodes]) #creates list of users from highest to lowest betweenness score for the color map\n",
    "#nx.draw(G, node_color=n_color, cmap=\"cool\", node_size=[math.log(v+1/2,10) * 2 for v in Betweenness_dict.values()]) #draws the undirected network using a color map and also adjusts node size by the log of the betweenness score + 1\n",
    "\n",
    "network_analysis_df = pd.DataFrame() #creates dataframe to output analysis\n",
    "network_analysis_df[\"Nodes\"] = Nodes #column 1 inlcudes each node name \n",
    "network_analysis_df[\"In-Degree\"] = Indegrees #column 2 inlcudes each node's in-degree \n",
    "network_analysis_df[\"Out-Degree\"] = Outdegrees #column 3 inlcudes each node's out-degree \n",
    "network_analysis_df[\"Betweenness\"] = Betweenness_dict.values() #column 4 inlcudes each node's betweenness score\n",
    "network_analysis_df.to_excel(\"output.xlsx\") #outputs the dataframe to an excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700fadaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4d929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
