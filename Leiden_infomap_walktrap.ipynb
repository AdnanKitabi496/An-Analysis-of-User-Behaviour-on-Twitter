{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LVhLYVXgSCK4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cdlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-31d9bb7687ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#!pip install cdlib  # Uncomment this line to run the code first time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcdlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata_language_german\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"colab_3Dec.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cdlib'"
     ]
    }
   ],
   "source": [
    "#!pip install cdlib  # Uncomment this line to run the code first time \n",
    "from cdlib import algorithms\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "data_language_german = pd.read_csv(\"colab_3Dec.csv\")\n",
    "data_language_german = data_language_german.loc[~pd.isnull(data_language_german[\"Tweet_From\"])]\n",
    "data_language_german= data_language_german.loc[~pd.isnull(data_language_german[\"Screen Name\"])]\n",
    "data_language_german = data_language_german[[\"Tweet_From\",'Screen Name', 'Tweet Content']]\n",
    "\n",
    "data_language_german = data_language_german[[\"Tweet_From\",'Screen Name']]\n",
    "edges = []\n",
    "nodes = set()\n",
    "tf_from = [tf for tf in data_language_german[\"Tweet_From\"]]\n",
    "edges = zip(data_language_german['Screen Name'], data_language_german['Tweet_From'])\n",
    "nodes.update(tf_from)\n",
    "edges = list(set(tuple(edge) for edge in edges))\n",
    "nodes = list(set(nodes))\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UTqyGHMSVwv"
   },
   "outputs": [],
   "source": [
    "# Finding top 5 communities using Leiden algorithm\n",
    "leiden_coms = algorithms.leiden(G)\n",
    "top_5_leiden = leiden_coms.communities[0:5]\n",
    "print(nx_comm.modularity(G, leiden_coms.communities)) #56\n",
    "\n",
    "\n",
    "# Finding top 5 communities using Infomap algorithm\n",
    "info_coms = algorithms.infomap(G)\n",
    "top_5_info_coms = info_coms.communities[0:5]\n",
    "print(nx_comm.modularity(G, info_coms.communities)) #30\n",
    "\n",
    "# Finding top 5 communities using Walktrap\n",
    "walk_com = algorithms.walktrap(G)\n",
    "top_5_walk_com = walk_com.communities[0:5]\n",
    "print(nx_comm.modularity(G, walk_com.communities)) #memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3bIqN4KS9rG",
    "outputId": "53e4defe-4e07-46c7-a83a-d754991a472f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_5_walk_com' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d9104873fc71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Finding intersection between top_5_walk_com vs top_5_info_coms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mw1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop_5_walk_com\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop_5_info_coms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mnewlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'top_5_walk_com' is not defined"
     ]
    }
   ],
   "source": [
    "# Finding intersection between top_5_walk_com vs top_5_info_coms\n",
    "for w1 in top_5_walk_com:\n",
    "    for i1 in top_5_info_coms:\n",
    "        newlist = list((set(w1).intersection(set(i1))))\n",
    "    print(newlist)\n",
    "    print(len(newlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOH_j6YkTVEJ",
    "outputId": "5887c1c1-0a6d-4b61-eb7e-5b832dbf0a1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n",
      "[]\n",
      "0\n",
      "[]\n",
      "0\n",
      "[]\n",
      "0\n",
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for l1 in top_5_leiden:\n",
    "  for i1 in top_5_info_coms:\n",
    "    newlist1 = list((set(l1).intersection(set(i1))))\n",
    "  print(newlist1)\n",
    "  print(len(newlist1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uA0p2fF5TXQI",
    "outputId": "cd50f7cb-7e1b-46b9-c249-90c8bf2e8fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n",
      "['flypaperplane', 'Jumpsteady', 'RadlerInWI', 'RocketPuppy97', 'LKVRS', 'ategus', 'climate_climber', 'oooof69', 'kathleenmartsch', 'DrHirschberger', 'drunkonhugs', 'SaraSchurmann', 'witsch', '9domi99', 'emathion', 'MarCopter', 'Vereine_Leipzig', 'ladybundlebrent', 'KatzenHai', 'MartinaStraub2', 'feuserin', 'simply_staeffi', 'anfalas', 'jublackthorns', 'P4F_Leipzig', 'NKarryWeber1', 'techn0krat', 'nofre67', 'michael_s68', 'Psychologists4F', 'RikoSaikaVA', 'ronzpp', 'F_Schmiedeknech', 'old_stachi', 'DevsForFuture', 'mr_wartung', 'quacooo', 'MeuselSarah', 'CaroSchirmer', 'RoteKlimaKarte', 'Stoerenfried', 'SofiaGanter', 'aloilisia', 'Dispogott77', 'eliasp', 'Flychris881', 'BettinaP4F', 'Benedikt_W', 'officialNixbest', 'JoasScholz', 'alxaraya', 'snoopsmaus', 'YannaBadet', 'RaphaelFor', 'Klonkimon', 'H76Ja', 'realgiggls', 'KoelleForFuture', 'nixus_nrw', '_Janine_H_', 'Alert4_Alert4', 'Ranger2060', 'MarcSchrewe', 'schrotthaufen', 'XR_Aufstand', 'sff_koeln', 'MeerderWorter', 'ElkeU18', 'XRLueneburg', 'dennoch_berlin', 'Yvorbium', 'oldoldtreptow', 'ulimax73', 'Narytan_', 'gerhard_weisser', 'KapChanka', 'm4uric3_99', 'WeidenLukas', 'XR_Koeln', 'Architects4F', 'MRWOMM', 'formschub', 'HollowStars', 'FFFRatingen', 'JuliaWalterFilm', 'firetorti', 'sinnogy', 'paulinepauline', 'kernelpanic', 'Dioneira', 'FFF_Mannheim', 'ktschuett', 'VQuaschning', 'ethmog', 'ExergieAt', 'TanjaFrank73', 'MatthiasKanneg1', 'martin_grotzke', 'evelynhayn', 'Captain_Teach17', 'xrfrankfurt', 'SusanneKleiber', 'Ohmmacht_de', 'PTempestae', 'TonendesWasser', 'joshhri', 'palimondo', '4lugia', 'Sabine47871137', 'magnusmunter', 'P4FBonn', 'PhilippWillms', 'pfeisi', 'Thomag0', 'rodcarbal', 'nilsmielke', 'vardiade', 'BremenRebellion', 'MisterMuff', 'mideg', 'NetNhark', 'CarinaMK_', 'PolwerkRiedel', 'SvenjaAppuhn', 'FlorianHenig', 'skaeptisch', 'P4FChemnitz', 'SvenCykelgodt', 'b_friederike', 'I9NBuende', 'HeadSoccer_Main']\n",
      "131\n",
      "['blum_marianne', 'DietherAst']\n",
      "2\n",
      "['SReceiver']\n",
      "1\n",
      "['Crosshead16']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for l1 in top_5_leiden:\n",
    "  for w2 in top_5_walk_com:\n",
    "    newlist2 = list((set(l1).intersection(set(w2))))\n",
    "  print(newlist2)\n",
    "  print(len(newlist2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HikHJih7Uh5j"
   },
   "outputs": [],
   "source": [
    "from textblob_de import TextBlobDE as TextBlob\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def find_sentiment(df):\n",
    "    len_hash = []\n",
    "    df[\"cleaned Tweet\"] = \"\"\n",
    "    df[\"Polarity\"] = \"\"\n",
    "    df[\"Sentiment\"] = \"\"\n",
    "    for c1, com in df.iterrows():\n",
    "        df[\"cleaned Tweet\"][c1] = re.sub(r\"RT @\\w+:\",\"\", str(com[\"Tweet Content\"]))\n",
    "        df[\"cleaned Tweet\"][c1] = re.sub(r\"#[a-zA-Z0-9]+\",\"\",str(df[\"cleaned Tweet\"][c1]))\n",
    "        #hashtag = re.findall(r\"#[a-zA-Z0-9]+\",str(df[\"cleaned Tweet\"][c1]))\n",
    "        #df[\"cleaned Tweet\"][c1] = re.sub(com[\"Tweet Content\"].replace(str(tt),\"\")\n",
    "\n",
    "        #len_hash.append(len(hashtag))\n",
    "        blob = TextBlob(df[\"cleaned Tweet\"][c1]).sentiment\n",
    "        df[\"Polarity\"][c1] = list(blob)[0]\n",
    "        if (list(blob)[0] == 0):\n",
    "            df[\"Sentiment\"][c1] =\"Neutral\"\n",
    "        elif (list(blob)[0] > 0):\n",
    "            df[\"Sentiment\"][c1] =\"Positive\"\n",
    "        else:\n",
    "            df[\"Sentiment\"][c1] =\"Negative\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_1 = data_language_german[[\"Screen Name\",\"Tweet_From\",\"Tweet Content\"]]\n",
    "com_1 = data_language_german[data_language_german['Tweet_From'].isin(leiden_coms[0])]\n",
    "com_1_sent_df = find_sentiment(com_1)\n",
    "\n",
    "com_1_sent_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_2 = data_language_german[[\"Screen Name\",\"Tweet_From\",\"Tweet Content\"]]\n",
    "com_2 = data_language_german[data_language_german['Tweet_From'].isin(leiden_coms[1])]\n",
    "com_2_sent_df = find_sentiment(com_2)\n",
    "com_2_sent_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_3 = data_language_german[[\"Screen Name\",\"Tweet_From\",\"Tweet Content\"]]\n",
    "com_3 = data_language_german[data_language_german['Tweet_From'].isin(leiden_coms[2])]\n",
    "com_3_sent_df = find_sentiment(com_3)\n",
    "com_3_sent_df['Sentiment'].value_counts()# Community 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_4 = data_language_german[[\"Screen Name\",\"Tweet_From\",\"Tweet Content\"]]\n",
    "com_4 = data_language_german[data_language_german['Tweet_From'].isin(leiden_coms[3])]\n",
    "com_4_sent_df = find_sentiment(com_4)\n",
    "com_4_sent_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_5 = data_language_german[[\"Screen Name\",\"Tweet_From\",\"Tweet Content\"]]\n",
    "com_5 = data_language_german[data_language_german['Tweet_From'].isin(leiden_coms[4])]\n",
    "com_5_sent_df = find_sentiment(com_5)\n",
    "com_5_sent_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iteration_utilities import deepflatten\n",
    "top_5_com = list(leiden_coms[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_com = [item for sublist in top_5_com for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_com = data_language_german[[\"Screen Name\",\"Tweet_From\",\"Tweet Content\"]]\n",
    "top5_com = data_language_german[data_language_german['Tweet_From'].isin(top_5_com)]\n",
    "top5_com_sent_df = find_sentiment(top5_com)\n",
    "\n",
    "top5_com_sent_df['Sentiment'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Leiden_infomap_walktrap.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
